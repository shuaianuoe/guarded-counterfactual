# Guarded-counterfactual

This is the code repository for "Counterfactual Explanation at Will, with Zero Privacy Leakage".

The codes are build upon Carla, which is a popular python library to benchmark counterfactual explanations. 

In order to successfully run the code, you **must first successfully install Carla** (https://github.com/carla-recourse/CARLA) via pip

```
pip install carla-recourse
```

Besides Carla, the following packages are also required.

```
gower 0.1.2
pandas 1.5.3
numpy 1.23.0
scipy 1.11.1
```


First, we should also configure a configuration file. The exact meaning of each parameter has been noted in `config.yaml`.

The original dataset can be found in the `datasets` folder.

Note that all the scripts should run on Windows 10/Ubuntu 14.04 and later systems.

## Section 1:  test the counterfactual quality

We should first set privacy to False in `config.yaml`.

To test our proposed algorithms, run below script:
```
python test_batch.py
```

To test the compared methods (You can specify the methods you want to test in  `config.yaml`), run below script:
```
python test_competitors.py
```

To test our proposed streaming algorithms, run below script:
```
python test_stream.py
```

Testing other variants is also simple. For instance, if you want to test maxmin and maxsum, just run below script:
```
python test_max_min_sum.py
```

If you want to test package query, you just need to set cf_method='pq' in `config.yaml` and then run
```
python test_packqry.py
```
Please note that we have almost never used the source code for package query, as our framework uses inference instances collected at the client side during model serving does not require the use of RDBMS.

For other streaming variants, set cf_method='gops' or 'gsv' or 'dr' in `config.yaml` and then run
```
python test_stream.py
```

## Section 2: test defending model attacks

Set privacy to True in `config.yaml`.

Based on the guarded counterfactual, execute the following command to generate an adversary for attacking the model's training set:
```
python test_batch.py
```

Based on the counterfactuals generated by compared methods, execute the following command to generate an adversary for attacking the model's training set:
```
python test_competitors.py
```

Once the training set is prepared, run the script below:
```
python test_privacy.py
```

We can observe the performance of the surrogate model.
